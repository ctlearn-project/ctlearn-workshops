{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86229d1-d81c-44c8-9fdb-214ac6fecb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c1486-0427-47a6-8d6d-17af2a3c4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda list | grep ctapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543472c-79b5-4549-84a0-976365a2c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "! du -h ../data/simtel/*/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af252721",
   "metadata": {},
   "source": [
    "## Reducing the data\n",
    "The frist step is the reduction of simtel files to HDF5 files following the official CTAO data model. Several data levels can be stored within one file. As an example, we are storing here the calibrated waveforms (R1) and the integrated images (DL1a) and image parameters (DL1b). This operation is done run-wise and can be easily parallelize on a cluster using sbatch. An additional script can be found in \"../scripts/run_reduction_cluster.py\". The scripts used for this reduction needs to be adjusted according to the naming of the simtel file. It is recommended carefully check before a major reduction is processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbada97-1734-43ad-a810-bba9a51fe0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../scripts/run_reduction.py \\\n",
    "--input_dir ../data/simtel/gamma-diffuse/ \\\n",
    "--type gamma \\\n",
    "--config ../configs/ctapipe_standard_LST1_config.json \\\n",
    "--output_dir ../data/R1DL1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21681fcf",
   "metadata": {},
   "source": [
    "We are also reducing a couple of proton files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663df3e-6ca3-4311-b4b1-a2131732a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../scripts/run_reduction.py \\\n",
    "--input_dir ../data/simtel/proton/ \\\n",
    "--type proton \\\n",
    "--config ../configs/ctapipe_standard_LST1_config.json \\\n",
    "--output_dir ../data/R1DL1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69cf252",
   "metadata": {},
   "source": [
    "After production we can check the files in the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175bd586-1a14-4d40-9e9d-6e2a0a44095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! du -h ../data/R1DL1/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739ac93",
   "metadata": {},
   "source": [
    "## Merging the HDF5 files\n",
    "It is HIGHLY RECOMMENDED to merge the HDF5 files. Usually, the whole production is merged into 100 files, where we split 80 files into the training and validation set and reserve 20 files for the test set to build the instrumental response functions (IRFs). Working with merged files ensure a rather smooth handling of the training process with DL1DH+CTLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e4785-9166-4dd7-801e-81ba3a69e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /Users/tjarkmiener/Desktop/ctlearn_workshop/ctlearn-workshops/ctlearn_workshop2024/data/R1DL1_merged\n",
    "! python ../scripts/run_merger.py \\\n",
    "--input_dir ../data/R1DL1/ \\\n",
    "--pattern \"gamma*\" \\\n",
    "--num_outputfiles 2 \\\n",
    "--output_dir ../data/R1DL1_merged/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175024a-239b-4b19-a9b0-242fc0fe0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../scripts/run_merger.py \\\n",
    "--input_dir ../data/R1DL1/ \\\n",
    "--pattern \"proton*\" \\\n",
    "--num_outputfiles 3 \\\n",
    "--output_dir ../data/R1DL1_merged/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf0314-a8f0-40f8-9778-0a5da41776fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "! du -h ../data/R1DL1_merged/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d76b946",
   "metadata": {},
   "source": [
    "## Browsing through the HDF5 files via vitables\n",
    "Simply install vitables ($ pip install vitables) on your machine and use this convenient GUI to explore the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c28f8-3482-482d-a3b0-a6fe3a2ce29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! vitables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3f683-8402-4abb-88bf-301d0d28751c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
